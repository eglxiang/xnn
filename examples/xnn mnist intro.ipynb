{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into RAM in expected format and visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mnist import *\n",
    "from pprint import pprint\n",
    "random.seed(12345)\n",
    "\n",
    "dataset = load_dataset()\n",
    "print dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print dataset['X_test'].dtype, dataset['y_test'].dtype\n",
    "print dataset['X_test'].shape, dataset['y_test'].shape\n",
    "print dataset['y_test'][0]\n",
    "imshow(dataset['X_test'][0].reshape(28,28), cmap=cm.gray, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking data into batches in callable form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define generators\n",
    "# make a generator to yield a batch of data for training/validating\n",
    "class iterate_minibatches():\n",
    "    def __init__(self, dataset, batchsize, partition='train'):\n",
    "        self.dataset = dataset\n",
    "        self.batchsize = batchsize\n",
    "        self.partition = partition\n",
    "\n",
    "    def __call__(self):\n",
    "        inputs = self.dataset['X_'+self.partition]\n",
    "        targets = self.dataset['y_'+self.partition]\n",
    "        for start_idx in range(0, len(inputs) - self.batchsize + 1, self.batchsize):\n",
    "            excerpt = slice(start_idx, start_idx + self.batchsize)\n",
    "            batchdata = dict(\n",
    "                X=inputs[excerpt],\n",
    "                y=targets[excerpt]\n",
    "            )\n",
    "            yield batchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make a train batch iterator and get a batch from it\n",
    "trainbatchit = iterate_minibatches(dataset, BATCHSIZE, 'train')\n",
    "it = trainbatchit()\n",
    "batch = it.next()\n",
    "print batch.keys()\n",
    "print batch['X'].shape, batch['X'].dtype\n",
    "\n",
    "# Note: 'X' and 'y' are the inputs and labels that will be bound to model layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define an XNN model that is a container around layer graphs\n",
    "m = Model(\"MLP\")\n",
    "\n",
    "pprint(m.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This creates an MLP of two hidden layers of 800 units each, followed by\n",
    "# a softmax output layer of 10 units. It applies 20% dropout to the input\n",
    "# data and 50% dropout to the hidden layers.\n",
    "\n",
    "# Input layer, specifying the expected input shape of the network\n",
    "# (unspecified batchsize, 1 channel, 28 rows and 28 columns) and\n",
    "# linking it to the given Theano variable `input_var`, if any:\n",
    "\n",
    "l_in = m.add_layer(InputLayer((None, 1, 28, 28)))\n",
    "\n",
    "# Apply 20% dropout to the input data:\n",
    "l_in_drop = m.make_dropout_layer(l_in, p=0.2)\n",
    "\n",
    "# Add a stack of fully-connected layers of 800 units each with dropout\n",
    "l_stacktop = m.make_dense_drop_stack(l_in_drop, [800, 800], drop_p_list=[.5, .5])\n",
    "\n",
    "# Finally, we'll add the fully-connected output layer, of 10 softmax units:\n",
    "l_out = m.add_layer(DenseLayer(l_stacktop, num_units=10, nonlinearity=softmax), \"l_out\")\n",
    "\n",
    "pprint(m.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m.bind_input(l_in, 'X')\n",
    "m.bind_output(l_out, categorical_crossentropy, 'y')\n",
    "\n",
    "pprint(m.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show the model graph\n",
    "modelgraphimg = xnn.utils.draw_to_file(m, '/tmp/modelgraph.png')\n",
    "modelgraphimg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict outputs before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outs = m.predict(batch)\n",
    "print outs.keys()\n",
    "print outs['l_out'].shape\n",
    "print outs['l_out'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m.save_model('/tmp/model.pkl')\n",
    "m2 = Model(\"loaded model\")\n",
    "m2.load_model('/tmp/model.pkl')\n",
    "outs2 = m2.predict(batch)\n",
    "print outs['l_out'][0] == outs2['l_out'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first set up global parameters for nesterov momentum\n",
    "global_update_settings = ParamUpdateSettings(\n",
    "    update=nesterov_momentum, learning_rate=0.25, momentum=0.9)\n",
    "\n",
    "# instantiate a trainer\n",
    "trainer = Trainer(m, global_update_settings)\n",
    "pprint(trainer.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run some training steps on a batch and modify updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set_printoptions(precision=3, suppress=True)\n",
    "print batch['y'][0]\n",
    "for i in range(5):\n",
    "    trainer.train_step(batch)\n",
    "    outs = m.predict(batch)\n",
    "    print outs['l_out'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# decrease learning rate and continue training\n",
    "trainer.bind_global_update(ParamUpdateSettings(learning_rate=0.1))\n",
    "for i in range(10):\n",
    "    trainer.train_step(batch)\n",
    "    outs = m.predict(batch)\n",
    "    print outs['l_out'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add l2-regularization with weight .001 to weights to all layers\n",
    "trainer.bind_regularization(xnn.regularization.l2, .001)\n",
    "\n",
    "for i in range(5):\n",
    "    trainer.train_step(batch)\n",
    "    outs = m.predict(batch)\n",
    "    print outs['l_out'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's start the batch iteration from scratch and re-initialize the model\n",
    "trainbatchit = iterate_minibatches(dataset, BATCHSIZE, 'train')\n",
    "validbatchit = iterate_minibatches(dataset, BATCHSIZE, 'valid')\n",
    "\n",
    "# use a convenience function defined in mnist.py to build the same mlp as above \n",
    "m = build_mlp()\n",
    "trainer.set_model(m)\n",
    "\n",
    "# define some metrics to keep track of performance\n",
    "metrics = [\n",
    "    ('l_out', Metric(computeCategoricalCrossentropy, \"y\", aggregation_type=\"mean\"), 'min'),\n",
    "    ('l_out', Metric(computeOneHotAccuracy, \"y\", aggregation_type=\"none\"), 'max')\n",
    "]\n",
    "\n",
    "# create a training loop\n",
    "loop = Loop(trainer, trainbatchit, validbatchit, metrics, plotmetricmean=False)\n",
    "\n",
    "# iterate through 3 epochs of training on all data\n",
    "loop(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First let's create a function for building an MLP from arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ##################### Build the neural network model #######################\n",
    "# We define a function that takes a Theano variable representing the input and returns\n",
    "# the output layer of a neural network model.\n",
    "def build_mlp(input_var=None, numhidunits=800, hiddropout=.5, dropout_type='standard'):\n",
    "    m = Model(\"MLP\")\n",
    "    # This creates an MLP of two hidden layers of 800 units each, followed by\n",
    "    # a softmax output layer of 10 units. It applies 20% dropout to the input\n",
    "    # data and 50% dropout to the hidden layers.\n",
    "\n",
    "    # Input layer, specifying the expected input shape of the network\n",
    "    # (unspecified batchsize, 1 channel, 28 rows and 28 columns) and\n",
    "    # linking it to the given Theano variable `input_var`, if any:\n",
    "\n",
    "    lin = m.make_bound_input_layer((None, 1, 28, 28), 'X', input_var=input_var)\n",
    "\n",
    "    # Apply 20% dropout to the input data:\n",
    "    l_in_drop = m.make_dropout_layer(lin, p=0.2)\n",
    "\n",
    "    # Add a stack of fully-connected layers of 800 units each with dropout\n",
    "    l_stacktop = m.make_dense_drop_stack(l_in_drop, [numhidunits, numhidunits],\n",
    "                                         drop_p_list=[hiddropout, hiddropout],drop_type_list=[dropout_type,dropout_type])\n",
    "\n",
    "    # Finally, we'll add the fully-connected output layer, of 10 softmax units:\n",
    "    l_out = m.add_layer(DenseLayer(l_stacktop, num_units=10, nonlinearity=softmax), \"l_out\")\n",
    "\n",
    "    m.bind_output(l_out, categorical_crossentropy, 'y')\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the variables that are relevant for exporing hyperparameter space in this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Cond(xnn.experiments.ExperimentCondition):\n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.1\n",
    "        self.hiddenunits   = 500\n",
    "        self.droptype      = 'standard' \n",
    "        self.hiddropout    = 0.5\n",
    "\n",
    "cond = Cond()\n",
    "print cond.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the experiment design (this one is a nested design with groups for std drop and gauss drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def set_up_experiment():\n",
    "    expt = xnn.experiments.Experiment(name='mnist mlp',default_condition=Cond())\n",
    "    expt.add_group('std drop')\n",
    "    expt.add_group('gauss drop')\n",
    "    expt.add_factor('learning_rate',[0.001,0.1])\n",
    "    expt.add_factor('hiddenunits',[20,200,500])\n",
    "    expt.add_factor('droptype','gauss',groupname='gauss drop')\n",
    "    expt.add_factor('hiddropout',[.3,1.5],groupname='gauss drop')\n",
    "    expt.add_factor('hiddropout',[.3,.75],groupname='std drop')\n",
    "    return expt\n",
    "\n",
    "expt = set_up_experiment()\n",
    "print expt.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train all experiment conditions for one epoch each and then report results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    metrics = [\n",
    "        ('l_out', Metric(computeCategoricalCrossentropy, \"y\", aggregation_type=\"mean\"), 'min'),\n",
    "        ('l_out', Metric(computeOneHotAccuracy, \"y\", aggregation_type=\"mean\"), 'max')\n",
    "    ]\n",
    "\n",
    "    trainbatchit = iterate_minibatches(dataset, BATCHSIZE, 'train')\n",
    "    validbatchit = iterate_minibatches(dataset, BATCHSIZE, 'valid')\n",
    "\n",
    "    #--------\n",
    "    # Run all conditions in experiment, and store results\n",
    "    #--------\n",
    "\n",
    "    for conddict in expt.get_all_condition_iterator():\n",
    "        print \"\\nRunning condition %d\\n\"%conddict['condition_num']\n",
    "        c = conddict['condition']\n",
    "        m = build_mlp(numhidunits=c.hiddenunits,hiddropout=c.hiddropout,dropout_type=c.droptype)\n",
    "        t = set_up_trainer(m,learning_rate=c.learning_rate)\n",
    "        loop = xnn.training.Loop(t,trainbatchit,validbatchit,metrics,plotmetricmean=False)\n",
    "        metvals = loop(1)\n",
    "        expt.add_results(conddict['condition_num'],metvals)\n",
    "\n",
    "\n",
    "    #--------\n",
    "    # Interpret results\n",
    "    #--------\n",
    "\n",
    "    #get results for standard vs gaussian dropout\n",
    "    std_nums = expt.get_condition_numbers(fixed_dict={'droptype':'standard'})\n",
    "    gss_nums = expt.get_condition_numbers(fixed_dict={'droptype':'gauss'})\n",
    "\n",
    "    print \"Standard dropout\"\n",
    "    bsp = None\n",
    "    for sn in std_nums:\n",
    "        cc,pc = expt.results[sn]\n",
    "        bsp = pc if (bsp is None or pc > bsp) else bsp\n",
    "        print \"%d: %0.3f\"%(sn,pc)  \n",
    "    print \"Gaussian dropout\"\n",
    "    bgp = None\n",
    "    for sn in gss_nums:\n",
    "        cc,pc = expt.results[sn]\n",
    "        bgp = pc if (bgp is None or pc > bgp) else bgp\n",
    "        print \"%d: %0.3f\"%(sn,pc)  \n",
    "\n",
    "    print \"Best Standard PC: %0.3f\"%bsp\n",
    "    print \"Best Gaussian PC: %0.3f\"%bgp\n",
    "\n",
    "    #get results for hidden unit size \n",
    "    bhu = [None]*3\n",
    "    for i,hu in enumerate([20,200,500]):\n",
    "        print \"%d hidden units\"%hu\n",
    "        nums = expt.get_condition_numbers(fixed_dict={'hiddenunits':hu})\n",
    "        for sn in nums:\n",
    "            cc,pc = expt.results[sn]\n",
    "            bhu[i] = pc if (bhu[i] is None or pc > bhu[i]) else bhu[i] \n",
    "            print \"%d: %0.3f\"%(sn,pc)  \n",
    "    for bh,hu  in zip(bhu,[20,200,500]):\n",
    "        print \"Best %d HU PC: %0.3f\"%(hu,bh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
