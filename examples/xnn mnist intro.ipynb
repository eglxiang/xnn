{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into RAM in expected format and visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "['X_test', 'X_train', 'y_train', 'X_valid', 'y_valid', 'y_test']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 750M\n"
     ]
    }
   ],
   "source": [
    "from mnist import *\n",
    "from pprint import pprint\n",
    "random.seed(12345)\n",
    "\n",
    "dataset = load_dataset()\n",
    "print dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 float32\n",
      "(10000, 1, 28, 28) (10000, 10)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10f7eff50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD8CAYAAABTq8lnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f39ca10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print dataset['X_test'].dtype, dataset['y_test'].dtype\n",
    "print dataset['X_test'].shape, dataset['y_test'].shape\n",
    "print dataset['y_test'][0]\n",
    "imshow(dataset['X_test'][0].reshape(28,28), cmap=cm.gray, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking data into batches in callable form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define generators\n",
    "# make a generator to yield a batch of data for training/validating\n",
    "class iterate_minibatches():\n",
    "    def __init__(self, dataset, batchsize, partition='train'):\n",
    "        self.dataset = dataset\n",
    "        self.batchsize = batchsize\n",
    "        self.partition = partition\n",
    "\n",
    "    def __call__(self):\n",
    "        inputs = self.dataset['X_'+self.partition]\n",
    "        targets = self.dataset['y_'+self.partition]\n",
    "        for start_idx in range(0, len(inputs) - self.batchsize + 1, self.batchsize):\n",
    "            excerpt = slice(start_idx, start_idx + self.batchsize)\n",
    "            batchdata = dict(\n",
    "                X=inputs[excerpt],\n",
    "                y=targets[excerpt]\n",
    "            )\n",
    "            yield batchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['y', 'X']\n",
      "(500, 1, 28, 28) float32\n"
     ]
    }
   ],
   "source": [
    "# make a train batch iterator and get a batch from it\n",
    "trainbatchit = iterate_minibatches(dataset, BATCHSIZE, 'train')\n",
    "it = trainbatchit()\n",
    "batch = it.next()\n",
    "print batch.keys()\n",
    "print batch['X'].shape, batch['X'].dtype\n",
    "\n",
    "# Note: 'X' and 'y' are the inputs and labels that will be bound to model layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_outputs': OrderedDict(),\n",
      " 'inputs': OrderedDict(),\n",
      " 'layers': [],\n",
      " 'name': 'MLP',\n",
      " 'outputs': OrderedDict()}\n"
     ]
    }
   ],
   "source": [
    "# Define an XNN model that is a container around layer graphs\n",
    "m = Model(\"MLP\")\n",
    "\n",
    "pprint(m.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_outputs': OrderedDict(),\n",
      " 'inputs': OrderedDict(),\n",
      " 'layers': [{'layer_type': 'InputLayer',\n",
      "             'name': 'InputLayer_0',\n",
      "             'output_shape': (None, 1, 28, 28),\n",
      "             'shape': (None, 1, 28, 28)},\n",
      "            {'incoming': 'InputLayer_0',\n",
      "             'input_shape': (None, 1, 28, 28),\n",
      "             'layer_type': 'DropoutLayer',\n",
      "             'name': 'DropoutLayer_0',\n",
      "             'output_shape': (None, 1, 28, 28),\n",
      "             'p': 0.2,\n",
      "             'rescale': True},\n",
      "            {'incoming': 'DropoutLayer_0',\n",
      "             'input_shape': (None, 1, 28, 28),\n",
      "             'layer_type': 'DenseLayer',\n",
      "             'name': 'DenseLayer_0',\n",
      "             'nonlinearity': 'rectify',\n",
      "             'num_units': 800,\n",
      "             'output_shape': (None, 800)},\n",
      "            {'incoming': 'DenseLayer_0',\n",
      "             'input_shape': (None, 800),\n",
      "             'layer_type': 'DropoutLayer',\n",
      "             'name': 'DropoutLayer_1',\n",
      "             'output_shape': (None, 800),\n",
      "             'p': 0.5,\n",
      "             'rescale': True},\n",
      "            {'incoming': 'DropoutLayer_1',\n",
      "             'input_shape': (None, 800),\n",
      "             'layer_type': 'DenseLayer',\n",
      "             'name': 'DenseLayer_1',\n",
      "             'nonlinearity': 'rectify',\n",
      "             'num_units': 800,\n",
      "             'output_shape': (None, 800)},\n",
      "            {'incoming': 'DenseLayer_1',\n",
      "             'input_shape': (None, 800),\n",
      "             'layer_type': 'DropoutLayer',\n",
      "             'name': 'DropoutLayer_2',\n",
      "             'output_shape': (None, 800),\n",
      "             'p': 0.5,\n",
      "             'rescale': True},\n",
      "            {'incoming': 'DropoutLayer_2',\n",
      "             'input_shape': (None, 800),\n",
      "             'layer_type': 'DenseLayer',\n",
      "             'name': 'l_out',\n",
      "             'nonlinearity': 'softmax',\n",
      "             'num_units': 10,\n",
      "             'output_shape': (None, 10)}],\n",
      " 'name': 'MLP',\n",
      " 'outputs': OrderedDict()}\n"
     ]
    }
   ],
   "source": [
    "# This creates an MLP of two hidden layers of 800 units each, followed by\n",
    "# a softmax output layer of 10 units. It applies 20% dropout to the input\n",
    "# data and 50% dropout to the hidden layers.\n",
    "\n",
    "# Input layer, specifying the expected input shape of the network\n",
    "# (unspecified batchsize, 1 channel, 28 rows and 28 columns) and\n",
    "# linking it to the given Theano variable `input_var`, if any:\n",
    "\n",
    "l_in = m.add_layer(InputLayer((None, 1, 28, 28)))\n",
    "\n",
    "# Apply 20% dropout to the input data:\n",
    "l_in_drop = m.make_dropout_layer(l_in, p=0.2)\n",
    "\n",
    "# Add a stack of fully-connected layers of 800 units each with dropout\n",
    "l_stacktop = m.make_dense_drop_stack(l_in_drop, [800, 800], drop_p_list=[.5, .5])\n",
    "\n",
    "# Finally, we'll add the fully-connected output layer, of 10 softmax units:\n",
    "l_out = m.add_layer(DenseLayer(l_stacktop, num_units=10, nonlinearity=softmax), \"l_out\")\n",
    "\n",
    "pprint(m.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_outputs': OrderedDict(),\n",
      " 'inputs': OrderedDict([('X', ['InputLayer_0'])]),\n",
      " 'layers': [{'layer_type': 'InputLayer',\n",
      "             'name': 'InputLayer_0',\n",
      "             'output_shape': (None, 1, 28, 28),\n",
      "             'shape': (None, 1, 28, 28)},\n",
      "            {'incoming': 'InputLayer_0',\n",
      "             'input_shape': (None, 1, 28, 28),\n",
      "             'layer_type': 'DropoutLayer',\n",
      "             'name': 'DropoutLayer_0',\n",
      "             'output_shape': (None, 1, 28, 28),\n",
      "             'p': 0.2,\n",
      "             'rescale': True},\n",
      "            {'incoming': 'DropoutLayer_0',\n",
      "             'input_shape': (None, 1, 28, 28),\n",
      "             'layer_type': 'DenseLayer',\n",
      "             'name': 'DenseLayer_0',\n",
      "             'nonlinearity': 'rectify',\n",
      "             'num_units': 800,\n",
      "             'output_shape': (None, 800)},\n",
      "            {'incoming': 'DenseLayer_0',\n",
      "             'input_shape': (None, 800),\n",
      "             'layer_type': 'DropoutLayer',\n",
      "             'name': 'DropoutLayer_1',\n",
      "             'output_shape': (None, 800),\n",
      "             'p': 0.5,\n",
      "             'rescale': True},\n",
      "            {'incoming': 'DropoutLayer_1',\n",
      "             'input_shape': (None, 800),\n",
      "             'layer_type': 'DenseLayer',\n",
      "             'name': 'DenseLayer_1',\n",
      "             'nonlinearity': 'rectify',\n",
      "             'num_units': 800,\n",
      "             'output_shape': (None, 800)},\n",
      "            {'incoming': 'DenseLayer_1',\n",
      "             'input_shape': (None, 800),\n",
      "             'layer_type': 'DropoutLayer',\n",
      "             'name': 'DropoutLayer_2',\n",
      "             'output_shape': (None, 800),\n",
      "             'p': 0.5,\n",
      "             'rescale': True},\n",
      "            {'incoming': 'DropoutLayer_2',\n",
      "             'input_shape': (None, 800),\n",
      "             'layer_type': 'DenseLayer',\n",
      "             'name': 'l_out',\n",
      "             'nonlinearity': 'softmax',\n",
      "             'num_units': 10,\n",
      "             'output_shape': (None, 10)}],\n",
      " 'name': 'MLP',\n",
      " 'outputs': OrderedDict([('l_out', {'aggregation_type': 'mean', 'scale': 1.0, 'output_layer': 'l_out', 'weight_key': None, 'target': 'y', 'target_type': 'label', 'loss_function': 'categorical_crossentropy'})])}\n"
     ]
    }
   ],
   "source": [
    "m.bind_input(l_in, 'X')\n",
    "m.bind_output(l_out, categorical_crossentropy, 'y')\n",
    "\n",
    "pprint(m.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show the model graph\n",
    "modelgraphimg = xnn.utils.draw_to_file(m, '/tmp/modelgraph.png')\n",
    "modelgraphimg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict outputs before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DenseLayer_1', 'DenseLayer_0', 'InputLayer_0', 'DropoutLayer_2', 'DropoutLayer_1', 'DropoutLayer_0', 'l_out']\n",
      "(500, 10)\n",
      "[ 0.05717565  0.11806224  0.11426906  0.12122438  0.06255974  0.13076603\n",
      "  0.08849301  0.10443016  0.10315739  0.09986235]\n"
     ]
    }
   ],
   "source": [
    "outs = m.predict(batch)\n",
    "print outs.keys()\n",
    "print outs['l_out'].shape\n",
    "print outs['l_out'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "m.save_model('/tmp/model.pkl')\n",
    "m2 = Model(\"loaded model\")\n",
    "m2.load_model('/tmp/model.pkl')\n",
    "outs2 = m2.predict(batch)\n",
    "print outs['l_out'][0] == outs2['l_out'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'global_update_settings': {'settings': {'learning_rate': 0.25,\n",
      "                                         'momentum': 0.9},\n",
      "                            'update': 'nesterov_momentum'},\n",
      " 'layer_updates': {},\n",
      " 'model': 'MLP',\n",
      " 'regularizations': {}}\n"
     ]
    }
   ],
   "source": [
    "# first set up global parameters for nesterov momentum\n",
    "global_update_settings = ParamUpdateSettings(\n",
    "    update=nesterov_momentum, learning_rate=0.25, momentum=0.9)\n",
    "\n",
    "# instantiate a trainer\n",
    "trainer = Trainer(m, global_update_settings)\n",
    "pprint(trainer.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run some training steps on a batch and modify updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "[ 0.216  0.078  0.026  0.106  0.099  0.08   0.074  0.057  0.1    0.163]\n",
      "[ 0.025  0.097  0.056  0.284  0.049  0.118  0.076  0.103  0.104  0.087]\n",
      "[ 0.08   0.062  0.047  0.242  0.044  0.127  0.065  0.084  0.123  0.127]\n",
      "[ 0.042  0.026  0.015  0.598  0.012  0.097  0.022  0.052  0.086  0.05 ]\n",
      "[ 0.016  0.016  0.014  0.458  0.007  0.217  0.02   0.029  0.154  0.068]\n"
     ]
    }
   ],
   "source": [
    "set_printoptions(precision=3, suppress=True)\n",
    "print batch['y'][0]\n",
    "for i in range(5):\n",
    "    trainer.train_step(batch)\n",
    "    outs = m.predict(batch)\n",
    "    print outs['l_out'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.006  0.003  0.004  0.647  0.001  0.224  0.004  0.013  0.076  0.021]\n",
      "[ 0.003  0.001  0.001  0.73   0.     0.208  0.001  0.003  0.046  0.006]\n",
      "[ 0.002  0.     0.001  0.727  0.     0.237  0.     0.001  0.029  0.003]\n",
      "[ 0.002  0.     0.001  0.782  0.     0.173  0.     0.001  0.04   0.001]\n",
      "[ 0.001  0.     0.     0.794  0.     0.194  0.     0.     0.011  0.   ]\n",
      "[ 0.     0.     0.     0.899  0.     0.095  0.     0.     0.006  0.   ]\n",
      "[ 0.     0.     0.     0.832  0.     0.16   0.     0.     0.008  0.   ]\n",
      "[ 0.     0.     0.     0.833  0.     0.143  0.     0.     0.024  0.   ]\n",
      "[ 0.     0.     0.     0.688  0.     0.301  0.     0.     0.011  0.   ]\n",
      "[ 0.     0.     0.     0.482  0.     0.511  0.     0.     0.007  0.   ]\n"
     ]
    }
   ],
   "source": [
    "# decrease learning rate and continue training\n",
    "trainer.bind_global_update(ParamUpdateSettings(learning_rate=0.1))\n",
    "for i in range(10):\n",
    "    trainer.train_step(batch)\n",
    "    outs = m.predict(batch)\n",
    "    print outs['l_out'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.     0.     0.     0.807  0.     0.185  0.     0.     0.008  0.   ]\n",
      "[ 0.001  0.     0.     0.597  0.     0.392  0.     0.     0.01   0.   ]\n",
      "[ 0.     0.     0.     0.279  0.     0.713  0.     0.     0.007  0.   ]\n",
      "[ 0.001  0.     0.     0.27   0.     0.722  0.     0.     0.006  0.   ]\n",
      "[ 0.001  0.     0.001  0.331  0.     0.655  0.     0.     0.011  0.   ]\n"
     ]
    }
   ],
   "source": [
    "# add l2-regularization with weight .001 to weights to all layers\n",
    "trainer.bind_regularization(xnn.regularization.l2, .001)\n",
    "\n",
    "for i in range(5):\n",
    "    trainer.train_step(batch)\n",
    "    outs = m.predict(batch)\n",
    "    print outs['l_out'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Epoch 0 / 2 -- 3.87 seconds\n",
      "Expected Finish: Wed, 12:17:34 PM\n",
      "----------\n",
      "Training total cost                                               :   2.0118 (best 2.0118 at epoch 0)\n",
      "----------\n",
      "Categorical Crossentropy                      l_out               :   0.2853 (best 0.2853 at epoch 0)\n",
      "Percent Correct                               l_out               :   0.9200 (best 0.9200 at epoch 0)\n",
      "==========\n",
      "Epoch 1 / 2 -- 2.13 seconds\n",
      "Expected Finish: Wed, 12:17:31 PM\n",
      "----------\n",
      "Training total cost                                               :   1.2873 (best 1.2873 at epoch 1)\n",
      "----------\n",
      "Categorical Crossentropy                      l_out               :   0.2105 (best 0.2105 at epoch 1)\n",
      "Percent Correct                               l_out               :   0.9400 (best 0.9400 at epoch 1)\n",
      "==========\n",
      "Epoch 2 / 2 -- 2.19 seconds\n",
      "Expected Finish: Wed, 12:17:30 PM\n",
      "----------\n",
      "Training total cost                                               :   0.9448 (best 0.9448 at epoch 2)\n",
      "----------\n",
      "Categorical Crossentropy                      l_out               :   0.1726 (best 0.1726 at epoch 2)\n",
      "Percent Correct                               l_out               :   0.9520 (best 0.9520 at epoch 2)\n",
      "Finished 3 epochs at 12:17:30 PM\n"
     ]
    }
   ],
   "source": [
    "# let's start the batch iteration from scratch and re-initialize the model\n",
    "trainbatchit = iterate_minibatches(dataset, BATCHSIZE, 'train')\n",
    "validbatchit = iterate_minibatches(dataset, BATCHSIZE, 'valid')\n",
    "\n",
    "# use a convenience function defined in mnist.py to build the same mlp as above \n",
    "m = build_mlp()\n",
    "trainer.set_model(m)\n",
    "\n",
    "# define some metrics to keep track of performance\n",
    "metrics = [\n",
    "    ('l_out', Metric(computeCategoricalCrossentropy, \"y\", aggregation_type=\"mean\"), 'min'),\n",
    "    ('l_out', Metric(computeOneHotAccuracy, \"y\", aggregation_type=\"none\"), 'max')\n",
    "]\n",
    "\n",
    "# create a training loop\n",
    "loop = Loop(trainer, trainbatchit, validbatchit, metrics, plotmetricmean=False)\n",
    "\n",
    "# iterate through 3 epochs of training on all data\n",
    "loop(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'default_condition': {'hiddropout': 0.5, 'numhid': 100},\n",
      " 'factors': {'hiddropout': [0.0, 0.1, 0.25, 0.5], 'numhid': [100, 400, 800]}}\n",
      "\n",
      "num conditions: 12\n",
      "conditions:\n",
      "{0: {'hiddropout': 0.0, 'numhid': 100},\n",
      " 1: {'hiddropout': 0.1, 'numhid': 100},\n",
      " 2: {'hiddropout': 0.25, 'numhid': 100},\n",
      " 3: {'hiddropout': 0.5, 'numhid': 100},\n",
      " 4: {'hiddropout': 0.0, 'numhid': 400},\n",
      " 5: {'hiddropout': 0.1, 'numhid': 400},\n",
      " 6: {'hiddropout': 0.25, 'numhid': 400},\n",
      " 7: {'hiddropout': 0.5, 'numhid': 400},\n",
      " 8: {'hiddropout': 0.0, 'numhid': 800},\n",
      " 9: {'hiddropout': 0.1, 'numhid': 800},\n",
      " 10: {'hiddropout': 0.25, 'numhid': 800},\n",
      " 11: {'hiddropout': 0.5, 'numhid': 800}}\n"
     ]
    }
   ],
   "source": [
    "from xnn.experiments import *\n",
    "# make an experiment condition class to store default variable values\n",
    "class _Condition(ExperimentCondition):\n",
    "    def __init__(self):\n",
    "        self.numhid = 100\n",
    "        self.hiddropout = .5\n",
    "\n",
    "# add an experiment with a numhid factor testing several levels\n",
    "expt = Experiment(\"numhid experiment\", _Condition())\n",
    "expt.add_factor(\"numhid\", [100, 400, 800])\n",
    "expt.add_factor(\"hiddropout\", [0., .1, .25, .5])\n",
    "\n",
    "pprint(expt.to_dict())\n",
    "\n",
    "print '\\nnum conditions:', expt.get_num_conditions()\n",
    "print 'conditions:'\n",
    "pprint(expt.get_all_conditions_changes())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hiddropout': 0.0, 'numhid': 100}\n",
      "{'hiddropout': 0.1, 'numhid': 100}\n",
      "{'hiddropout': 0.25, 'numhid': 100}\n",
      "{'hiddropout': 0.5, 'numhid': 100}\n",
      "{'hiddropout': 0.0, 'numhid': 400}\n",
      "{'hiddropout': 0.1, 'numhid': 400}\n",
      "{'hiddropout': 0.25, 'numhid': 400}\n",
      "{'hiddropout': 0.5, 'numhid': 400}\n",
      "{'hiddropout': 0.0, 'numhid': 800}\n",
      "{'hiddropout': 0.1, 'numhid': 800}\n",
      "{'hiddropout': 0.25, 'numhid': 800}\n",
      "{'hiddropout': 0.5, 'numhid': 800}\n"
     ]
    }
   ],
   "source": [
    "# iterate over all conditions\n",
    "for cond in expt.get_conditions_iterator():\n",
    "    pprint(cond.to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hiddropout': ('hiddropout', 0.5), 'numhid': 100}\n",
      "{'hiddropout': ('hiddropout', 0.5), 'numhid': 400}\n",
      "{'hiddropout': ('hiddropout', 0.5), 'numhid': 800}\n"
     ]
    }
   ],
   "source": [
    "# iterate over numhid, holding hiddropout fixed at 0.5\n",
    "for cond in expt.get_conditions_slice_iterator(['numhid'],{'hiddropout':0.5}):\n",
    "    pprint(cond.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
